from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_openai import ChatOpenAI
import json
import os
from dotenv import load_dotenv

load_dotenv()

def convert_chunks_to_documents(chunks):
    documents = []
    for chunk in chunks:
        documents.append(
            Document(
                page_content = chunk['content'],
                metadata={"source": chunk['source']}
            )
        )
    return documents

def evaluate_answer(user_question, system_answer, chunks):
    llm_rag = ChatOpenAI(
        model_name=os.getenv('LLM_MODEL'), 
        temperature=1
    )

    prompt_rag = ChatPromptTemplate.from_messages([
        ("system",
         "Evaluate the answer generated by a RAG system. "
         "Provide: "
         "1. A score from 0â€“10; "
         "2. A short justification; "
         "Return JSON."),

        ("human", 
         "USER QUESTION: {user_question}\n\n"
         "SYSTEM ANSWER: {system_answer}\n\n"
         "CONTEXT:\n{context}")
    ])

    document_chain = create_stuff_documents_chain(
        llm_rag,
        prompt_rag
    )

    answer = document_chain.invoke({
        'user_question': user_question,
        'system_answer': system_answer,
        'context': convert_chunks_to_documents(chunks)
    })

    return json.loads(answer.strip())

if __name__ == "__main__":
    with open('outputs/sample_queries.json', 'r') as f:
        sample_queries = json.load(f)

    for query in sample_queries:
        result = evaluate_answer(
            query['user_question'],
            query['system_answer'],
            query['chunks_related'])
        print(json.dumps(result, indent=2))